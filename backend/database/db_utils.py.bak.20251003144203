import logging
import numpy as np
import json
from base64 import b64encode, b64decode
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sklearn.metrics.pairwise import cosine_similarity
from alembic.config import Config
from alembic import command

from backend.core.config import settings
from backend.api.models import schemas
from backend.core.security import get_password_hash, KmsManager, envelope_encrypt, envelope_decrypt

logger = logging.getLogger(__name__)

kms_manager = KmsManager(settings.ENCRYPTION_KEY)

engine = create_engine(settings.DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def init_db():
    """
    Initializes the database, runs migrations, and creates initial users.
    """
    try:
        # Import models here to avoid circular imports
        from .models import User, IdentityEmbedding, AuditLog, Evidence, StreamSource, Snapshot
        
        # Create connection to DB
        try:
            conn = engine.connect()
            conn.close()
            logger.info("✅ Database connection successful")
        except Exception as e:
            logger.error(f"❌ Failed to connect to database: {str(e)}")
            logger.error(f"Database URL: {settings.DATABASE_URL}")
            raise
        
        # Create all tables if they don't exist
        logger.info("Creating database tables if they don't exist...")
        Base.metadata.create_all(bind=engine)
        
        logger.info("Running DB migrations...")
        try:
            alembic_cfg = Config("alembic.ini")
            command.upgrade(alembic_cfg, "head")
        except Exception as e:
            logger.error(f"Alembic migration failed: {str(e)}")
            logger.warning("Continuing with initialization despite migration failure")
        
        # Add dummy users if they don't exist
        db = SessionLocal()
        try:
            if not db.query(User).filter(User.username == settings.DUMMY_USER_USERNAME).first():
                logger.info(f"Creating dummy user: {settings.DUMMY_USER_USERNAME}")
                user = User(
                    username=settings.DUMMY_USER_USERNAME,
                    hashed_password=get_password_hash(settings.DUMMY_USER_PASSWORD),
                    role="officer"
                )
                db.add(user)
                db.commit()
            if not db.query(User).filter(User.username == settings.DUMMY_ADMIN_USERNAME).first():
                logger.info(f"Creating dummy admin: {settings.DUMMY_ADMIN_USERNAME}")
                admin = User(
                    username=settings.DUMMY_ADMIN_USERNAME,
                    hashed_password=get_password_hash(settings.DUMMY_ADMIN_PASSWORD),
                    role="admin"
                )
                db.add(admin)
                db.commit()
            logger.info("✅ DB initialization complete")
        except Exception as e:
            logger.error(f"Failed to create dummy users: {str(e)}")
            raise
        finally:
            db.close()
    except Exception as e:
        logger.error(f"❌ Database initialization failed: {str(e)}")
        raise

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
        
        
def encrypt_data(data: bytes) -> bytes:
    """
    Encrypts binary data using envelope encryption.
    
    Args:
        data: The binary data to encrypt
        
    Returns:
        Serialized encrypted package as bytes
    """
    try:
        # Use the new envelope_encrypt function
        encrypted_package = envelope_encrypt(data)
        
        # Convert the dictionary to JSON and then to bytes
        return json.dumps(encrypted_package).encode('utf-8')
    except Exception as e:
        logger.error(f"Error encrypting data: {str(e)}")
        # Fall back to legacy encryption if needed
        try:
            ciphertext_blob, encrypted_dek = kms_manager.encrypt(data)
            legacy_package = {
                "ciphertext_blob": b64encode(ciphertext_blob).decode('utf-8'),
                "encrypted_dek": b64encode(encrypted_dek).decode('utf-8'),
                "encryption_method": "LEGACY_KMS"
            }
            return json.dumps(legacy_package).encode('utf-8')
        except Exception as inner_e:
            logger.error(f"Critical error in encryption fallback: {str(inner_e)}")
            raise ValueError("Failed to encrypt data") from inner_e


def decrypt_data(encrypted_data: bytes) -> bytes:
    """
    Decrypts binary data that was encrypted using envelope encryption.
    
    Args:
        encrypted_data: The encrypted data as bytes
        
    Returns:
        The decrypted binary data
    """
    try:
        # Parse the encrypted package from JSON
        encrypted_package = json.loads(encrypted_data.decode('utf-8'))
        
        # Check which encryption method was used
        if encrypted_package.get("encryption_method") == "AESGCM_ENVELOPE":
            # Use the new envelope_decrypt function
            return envelope_decrypt(encrypted_package)
        elif encrypted_package.get("encryption_method") == "LEGACY_KMS":
            # Handle legacy encryption format
            ciphertext_blob = b64decode(encrypted_package["ciphertext_blob"])
            encrypted_dek = b64decode(encrypted_package["encrypted_dek"])
            return kms_manager.decrypt(ciphertext_blob, encrypted_dek)
        else:
            # Handle unknown encryption method
            raise ValueError(f"Unknown encryption method: {encrypted_package.get('encryption_method')}")
    except Exception as e:
        logger.error(f"Error decrypting data: {str(e)}")
        raise ValueError("Failed to decrypt data") from e

def store_identity_embedding(db: Session, identity_id: str, embedding: np.ndarray):
    """
    Stores a new identity embedding in the database.
    """
    # Import model locally to avoid circular imports
    from .models import IdentityEmbedding
    
    # Encrypt the embedding using envelope encryption
    encrypted_package = envelope_encrypt(embedding.tobytes())
    
    # Extract the encrypted data and DEK from the package
    encrypted_embedding = encrypted_package["encrypted_data"].encode()
    encrypted_dek = json.dumps(encrypted_package).encode()
    
    db_embedding = IdentityEmbedding(
        identity_id=identity_id, 
        embedding=encrypted_embedding,
        encrypted_dek=encrypted_dek,
        is_encrypted=True
    )
    db.add(db_embedding)
    db.commit()
    db.refresh(db_embedding)
    logger.info(f"Stored embedding for identity: {identity_id}")


def search_identities(query_embedding: np.ndarray, top_n: int = 5) -> list:
    """
    Searches the database for the most similar identity embeddings.

    Args:
        query_embedding: The embedding of the face to search for.
        top_n: The number of top matches to return.

    Returns:
        A list of dictionaries, each containing 'identity_id' and 'confidence'.
    """
    # Import model locally to avoid circular imports
    from .models import IdentityEmbedding
    
    with SessionLocal() as db:
        all_identities = db.query(IdentityEmbedding).all()

        if not all_identities:
            return []

        # Decrypt embeddings based on whether they use envelope encryption or not
        decrypted_embeddings = []
        for identity in all_identities:
            try:
                if identity.is_encrypted and identity.encrypted_dek:
                    # Use envelope decryption for newer records
                    encrypted_package = json.loads(identity.encrypted_dek.decode('utf-8'))
                    decrypted_data = envelope_decrypt(encrypted_package)
                else:
                    # Fallback for legacy data
                    decrypted_data = decrypt_data(identity.embedding)
                
                # Convert bytes to numpy array
                embedding = np.frombuffer(decrypted_data, dtype=np.float64)
                decrypted_embeddings.append(embedding)
            except Exception as e:
                logger.error(f"Error decrypting embedding for {identity.identity_id}: {e}")
                # Skip this embedding if decryption fails
                continue

        if not decrypted_embeddings:
            return []

        db_embeddings = np.array(decrypted_embeddings)
        query_embedding_reshaped = query_embedding.reshape(1, -1)

        similarities = cosine_similarity(query_embedding_reshaped, db_embeddings)

        # Need to adjust for any skipped embeddings
        valid_identities = [identity for i, identity in enumerate(all_identities) 
                          if i < len(similarities[0])]
        
        # Get top indices, but make sure we don't try to access beyond the valid indices
        top_n = min(top_n, len(valid_identities))
        top_indices = np.argsort(similarities[0])[::-1][:top_n]

        results = [{"identity_id": valid_identities[i].identity_id, 
                   "confidence": float(similarities[0][i])} 
                  for i in top_indices]
        return results

def get_user_by_username(db: Session, username: str):
    """Retrieves a user by their username."""
    # Import model locally to avoid circular imports
    from .models import User
    return db.query(User).filter(User.username == username).first()

def create_stream_source(db: Session, stream: schemas.StreamInput):
    """Creates a new CCTV stream source."""
    # Import model locally to avoid circular imports
    from .models import StreamSource
    db_stream = StreamSource(
        name=stream.name,
        rtsp_url=stream.rtsp_url,
        location=stream.location,
        is_active=stream.is_active
    )
    db.add(db_stream)
    db.commit()
    db.refresh(db_stream)
    return db_stream

def get_stream_sources(db: Session, skip: int = 0, limit: int = 100):
    """Retrieves all registered CCTV stream sources."""
    # Import model locally to avoid circular imports
    from .models import StreamSource
    return db.query(StreamSource).offset(skip).limit(limit).all()

def create_audit_log(db: Session, entry: dict):
    """Creates an audit log entry."""
    # Import model locally to avoid circular imports
    from .models import AuditLog
    log_entry = AuditLog(
        user=entry.get("user"),
        ip_address=entry.get("ip_address"),
        endpoint=entry.get("endpoint"),
        method=entry.get("method"),
        status_code=entry.get("status_code"),
        response_time_ms=entry.get("response_time_ms")
    )
    db.add(log_entry)
    db.commit()
    db.refresh(log_entry)
    return log_entry


def store_evidence(
    db: Session,
    file_content: bytes,
    file_name: str,
    media_type: str,
    evidence_type: str,
    description: str = None,
):
    """
    Stores multimodal evidence in the database with encrypted content.
    """
    # Import model locally to avoid circular imports
    from .models import Evidence
    
    # Encrypt the evidence content using envelope encryption
    encrypted_content = encrypt_data(file_content)
    
    evidence_entry = Evidence(
        file_name=file_name,
        media_type=media_type,
        evidence_type=evidence_type,
        description=description,
        content=encrypted_content,
    )
    db.add(evidence_entry)
    db.commit()
    db.refresh(evidence_entry)
    logger.info(f"Stored evidence {file_name} of type {evidence_type}")
    return evidence_entry.id
    
def get_evidence(db: Session, evidence_id: int):
    """
    Retrieves evidence from the database and decrypts its content.
    
    Args:
        db: Database session
        evidence_id: ID of the evidence to retrieve
        
    Returns:
        Tuple of (evidence, decrypted_content) or (evidence, None) if decryption fails
    """
    # Import model locally to avoid circular imports
    from .models import Evidence
    evidence = db.query(Evidence).filter(Evidence.id == evidence_id).first()
    if not evidence:
        logger.error(f"Evidence not found: {evidence_id}")
        return None, None
        
    try:
        decrypted_content = decrypt_data(evidence.content)
        return evidence, decrypted_content
    except Exception as e:
        logger.error(f"Error decrypting evidence {evidence_id}: {e}")
        return evidence, None

import sqlite3
import pickle

def store_snapshot(db: Session, snapshot_id: str, identity_id: str, location: str, image_data: bytes):
    """
    Stores a snapshot in the database with encrypted image data.
    
    Args:
        db: Database session
        snapshot_id: Unique ID for the snapshot
        identity_id: ID of the recognized identity
        location: Location where the snapshot was taken
        image_data: Raw image data bytes
        
    Returns:
        The created Snapshot object
    """
    # Import model locally to avoid circular imports
    from .models import Snapshot
    
    # Encrypt the image data using envelope encryption
    encrypted_package = envelope_encrypt(image_data)
    
    # Extract the encrypted data and DEK from the package
    encrypted_data = encrypted_package["encrypted_data"].encode()
    encrypted_dek = json.dumps(encrypted_package).encode()
    
    snapshot = Snapshot(
        id=snapshot_id,
        identity_id=identity_id,
        location=location,
        timestamp=datetime.utcnow(),
        encrypted_image_data=encrypted_data,
        encrypted_dek=encrypted_dek,
        is_encrypted=True
    )
    
    db.add(snapshot)
    db.commit()
    db.refresh(snapshot)
    logger.info(f"Stored snapshot for identity: {identity_id} at location: {location}")
    return snapshot

def get_snapshot(db: Session, snapshot_id: str):
    """
    Retrieves a snapshot from the database and decrypts the image data.
    
    Args:
        db: Database session
        snapshot_id: ID of the snapshot to retrieve
        
    Returns:
        Tuple of (snapshot, decrypted_image_data)
    """
    # Import model locally to avoid circular imports
    from .models import Snapshot
    snapshot = db.query(Snapshot).filter(Snapshot.id == snapshot_id).first()
    if not snapshot:
        logger.error(f"Snapshot not found: {snapshot_id}")
        return None, None
    
    try:
        if snapshot.is_encrypted and snapshot.encrypted_dek:
            # Use the envelope encryption/decryption
            encrypted_package = json.loads(snapshot.encrypted_dek.decode('utf-8'))
            decrypted_image_data = envelope_decrypt(encrypted_package)
        else:
            # Fallback for legacy data
            decrypted_image_data = decrypt_data(snapshot.encrypted_image_data)
        
        return snapshot, decrypted_image_data
    except Exception as e:
        logger.error(f"Error decrypting snapshot {snapshot_id}: {e}")
        return snapshot, None

def upsert_watchlist_identity(db: Session, identity: str, mean_vec: np.ndarray):
    """
    Stores or updates a watchlist identity's mean embedding in the database.
    """
    conn = sqlite3.connect(settings.FACES_EMBEDDINGS_DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS watchlist_means (
            id TEXT PRIMARY KEY,
            vec BLOB
        )
    """)
    
    # Serialize the numpy array to bytes
    vec_bytes = pickle.dumps(mean_vec)
    
    cursor.execute("""
        INSERT OR REPLACE INTO watchlist_means (id, vec)
        VALUES (?, ?)
    """, (identity, vec_bytes))
    conn.commit()
    conn.close()
    logger.info(f"Upserted watchlist identity: {identity}")

def load_watchlist_means(db: Session) -> dict[str, np.ndarray]:
    """
    Loads all watchlist identity mean embeddings from the database.
    """
    conn = sqlite3.connect(settings.FACES_EMBEDDINGS_DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT id, vec FROM watchlist_means")
    rows = cursor.fetchall()
    conn.close()
    
    watchlist_means = {}
    for identity_id, vec_bytes in rows:
        watchlist_means[identity_id] = pickle.loads(vec_bytes)
    
    logger.info(f"Loaded {len(watchlist_means)} watchlist identities.")
    return watchlist_means
