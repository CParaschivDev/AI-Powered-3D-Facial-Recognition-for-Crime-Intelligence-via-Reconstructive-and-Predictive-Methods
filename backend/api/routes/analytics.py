import logging
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, HTTPException, Query, Form, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from datetime import datetime, timedelta
from pathlib import Path
from backend.core.config import settings

from backend.api.models import schemas
from backend.database.dependencies import get_db
from backend.database.models import Prediction
from backend.analytics.predict import load_and_aggregate_crime_data, run_prediction_pipeline, CRIME_DATA_DIR
from backend.services.crime_context import build_parquet

router = APIRouter()
logger = logging.getLogger(__name__)

# In-memory build status for crime parquet
BUILD_STATUS = {
    "running": False,
    "started_at": None,
    "finished_at": None,
    "output_path": None,
    "size_bytes": None,
    "error": None,
    "files_total": None,
    "files_processed": 0,
    "rows_written": 0,
    "last_file": None,
}

@router.get(
    "/analytics/predictions",
    response_model=schemas.PredictionResponse,
    summary="Get crime predictions (national daily forecasts)",
)
async def get_predictions(
    window: int = Query(365, description="Number of days of predictions to retrieve.", gt=0, le=365),
    db: Session = Depends(get_db),
):
    """
    Retrieves national daily crime predictions.
    The predictions are generated by Prophet on aggregated daily time series.
    """
    logger.info(f"Requesting national predictions with window {window} days.")

    try:
        # Get future predictions (from tomorrow onwards)
        start_date = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
        end_date = start_date + timedelta(days=window-1)

        predictions = (
            db.query(Prediction)
            .filter(Prediction.area_id == 'NATIONAL',
                   Prediction.crime_type == 'ALL_CRIMES',
                   Prediction.ts >= start_date,
                   Prediction.ts <= end_date)
            .order_by(Prediction.ts.asc())
            .all()
        )

        logger.info(f"Retrieved {len(predictions)} national daily predictions")
        return schemas.PredictionResponse(predictions=predictions)

    except Exception as e:
        logger.exception("Failed to retrieve national predictions")
        raise HTTPException(status_code=500, detail="Error retrieving predictions.")

@router.post(
    "/analytics/run-predictions",
    status_code=200,
    summary="Trigger the national crime prediction pipeline",
)
async def trigger_predictions(
    db: Session = Depends(get_db),
):
    """
    Triggers the national crime prediction pipeline using daily aggregated time series.
    This processes raw CSV files and generates Prophet forecasts for 30-365 days ahead.
    """
    logger.info("Triggering national crime prediction pipeline.")
    try:
        from backend.analytics.predict import load_and_aggregate_crime_data, run_prediction_pipeline, CRIME_DATA_DIR

        crime_df = load_and_aggregate_crime_data(CRIME_DATA_DIR)
        if crime_df.empty:
            raise HTTPException(status_code=500, detail="Failed to load and aggregate crime data.")

        run_prediction_pipeline(db, crime_df)
        return {"message": "National crime prediction pipeline completed successfully."}

    except Exception as e:
        logger.exception("Failed to trigger national crime prediction pipeline.")
        raise HTTPException(status_code=500, detail=f"Error triggering prediction pipeline: {str(e)}")

@router.post("/analytics/crime/context", response_model=schemas.GenericOk)
async def build_crime_context(
    force: str | None = Form(None),
    background_tasks: BackgroundTasks = None,
):
    out = (Path(settings.DATA_ROOT).resolve() / "processed" / "crime_full.parquet") if settings.DATA_ROOT else Path("reports/crime/crime_full.parquet")
    try:
        if background_tasks is not None:
            def _build_task():
                try:
                    from datetime import datetime
                    import os
                    logger.info("Starting crime parquet build%s", f" (force filter: {force})" if force else "")
                    BUILD_STATUS.update({
                        "running": True,
                        "started_at": datetime.utcnow().isoformat() + "Z",
                        "finished_at": None,
                        "output_path": str(out),
                        "size_bytes": None,
                        "error": None,
                        "files_total": None,
                        "files_processed": 0,
                        "rows_written": 0,
                        "last_file": None,
                    })
                    # Progress callback to update status incrementally
                    def _progress(update: dict):
                        BUILD_STATUS.update(update)
                    path = build_parquet(out, force_filter=force, progress_callback=_progress)
                    size_bytes = os.path.getsize(path) if path and os.path.exists(path) else None
                    logger.info("Crime parquet built at %s (size=%s bytes)", path, size_bytes)
                    # Clear cached crime dataframe so new parquet is loaded on next request
                    try:
                        from backend.services import crime_service as _crime_service
                        _crime_service.clear_crime_caches()
                        logger.info("Cleared crime caches after build")
                    except Exception:
                        logger.exception("Failed clearing crime caches")
                    BUILD_STATUS.update({
                        "running": False,
                        "finished_at": datetime.utcnow().isoformat() + "Z",
                        "output_path": path,
                        "size_bytes": size_bytes,
                        "error": None,
                    })
                except Exception:
                    from datetime import datetime
                    logger.exception("Background crime parquet build failed")
                    BUILD_STATUS.update({
                        "running": False,
                        "finished_at": datetime.utcnow().isoformat() + "Z",
                        "error": "Build failed - see server logs",
                    })
            background_tasks.add_task(_build_task)
            return {"ok": True, "details": f"Building started: {out}"}
        # Fallback: synchronous build
        from datetime import datetime
        import os
        logger.info("Starting crime parquet build (sync)%s", f" (force filter: {force})" if force else "")
        BUILD_STATUS.update({
            "running": True,
            "started_at": datetime.utcnow().isoformat() + "Z",
            "finished_at": None,
            "output_path": str(out),
            "size_bytes": None,
            "error": None,
            "files_total": None,
            "files_processed": 0,
            "rows_written": 0,
            "last_file": None,
        })
        def _progress(update: dict):
            BUILD_STATUS.update(update)
        path = build_parquet(out, force_filter=force, progress_callback=_progress)
        size_bytes = os.path.getsize(path) if path and os.path.exists(path) else None
        BUILD_STATUS.update({
            "running": False,
            "finished_at": datetime.utcnow().isoformat() + "Z",
            "output_path": path,
            "size_bytes": size_bytes,
            "error": None,
        })
        # Clear cached crime dataframe so subsequent reads reflect new build
        try:
            from backend.services import crime_service as _crime_service
            _crime_service.clear_crime_caches()
            logger.info("Cleared crime caches after synchronous build")
        except Exception:
            logger.exception("Failed clearing crime caches (sync build)")
        return {"ok": bool(path), "details": str(path)}
    except ValueError as e:
        if "DATA_ROOT not configured" in str(e):
            raise HTTPException(status_code=500, detail="Configure DATA_ROOT in environment settings")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.exception("Error building crime context parquet file")
        raise HTTPException(status_code=500, detail=f"Failed to build crime context: {str(e)}")


@router.get("/analytics/crime/context/status")
async def get_crime_context_status():
    """Return current build status for crime parquet."""
    return {"ok": True, "status": BUILD_STATUS}




